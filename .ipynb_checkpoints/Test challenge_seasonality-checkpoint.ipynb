{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test challenge_seasonality by Javier Martinez\n",
    "\n",
    "For answering the requested case of study I have programmed a python file called \"BW7_toolbox.py\" that is attached in my submission. This file contains two functions: correlation_study and seasons_study. Each one is the answer for each of the question suggested in the case of study. In the same project there is also a __init__ file just for the sake of generating a python package.\n",
    "\n",
    "To execute the functions just unzip the file in the folder where regularly is used as a python path and use \"import BW7_toolbox as BW7\" to add it to your project. Once in your project you could call the evaluation of the solutions as BW7.correlation_study(filename) or BW7.seasons_study(filename). By default the data for this case of study will be loaded but any csv file that is in the same folder that the python files coudl be also used just by using the proper filename (with the csv extension as part of the name). \n",
    "\n",
    "All the plots during the execution will be executed in different windows to allow better insight/control of the plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "I would like to add this paragraph just to let you know the version of the pacakges used for this test. It is something I forgot but that could be critical for running and having similar results. \n",
    "\n",
    "* Python Interpreter 2.7.12\n",
    "* Numpy 1.11.3\n",
    "* Pandas 0.19.2\n",
    "* Matplotlib 1.5.1\n",
    "* Seaborn 0.7.1\n",
    "* Scipy 0.18.1\n",
    "* Statsmodels 0.8.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.signal as sgn\n",
    "import scipy.stats as stat\n",
    "import matplotlib.gridspec as grds\n",
    "\n",
    "\n",
    "import statsmodels.tsa.stattools as sta\n",
    "\n",
    "% matplotlib qt\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            singups  paidmedia_offline_spend  paidmedia_online_spend  \\\n",
      "event_date                                                             \n",
      "2014-01-01     4246                      NaN                41934.14   \n",
      "2014-01-02     6569                      NaN                54117.28   \n",
      "2014-01-03     7466                      NaN                51633.65   \n",
      "2014-01-04     6911                      NaN                47323.12   \n",
      "2014-01-05     5929                      NaN                48323.98   \n",
      "\n",
      "                holiday  \n",
      "event_date               \n",
      "2014-01-01  NewYearsDay  \n",
      "2014-01-02          NaN  \n",
      "2014-01-03          NaN  \n",
      "2014-01-04          NaN  \n",
      "2014-01-05          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv('challenge-data-v2.csv', index_col='event_date',parse_dates = True)\n",
    "\n",
    "\n",
    "# Taking a look into the first elements of the data to know more about the structure\n",
    "print data.head()\n",
    "\n",
    "# Changing the name of the colums to a more suitable (shorter) form\n",
    "data.columns = ['sups','moff','mon','hd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sups</th>\n",
       "      <th>moff</th>\n",
       "      <th>mon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1155.000000</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>1155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8054.577489</td>\n",
       "      <td>98884.865284</td>\n",
       "      <td>334811.893541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2093.419767</td>\n",
       "      <td>71107.830804</td>\n",
       "      <td>152979.099583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3973.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40243.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6657.500000</td>\n",
       "      <td>55285.487500</td>\n",
       "      <td>234232.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7639.000000</td>\n",
       "      <td>84413.085000</td>\n",
       "      <td>314120.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8967.500000</td>\n",
       "      <td>123233.000000</td>\n",
       "      <td>430718.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26348.000000</td>\n",
       "      <td>446191.350000</td>\n",
       "      <td>975192.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sups           moff            mon\n",
       "count   1155.000000     282.000000    1155.000000\n",
       "mean    8054.577489   98884.865284  334811.893541\n",
       "std     2093.419767   71107.830804  152979.099583\n",
       "min     3973.000000       0.000000   40243.090000\n",
       "25%     6657.500000   55285.487500  234232.175000\n",
       "50%     7639.000000   84413.085000  314120.800000\n",
       "75%     8967.500000  123233.000000  430718.990000\n",
       "max    26348.000000  446191.350000  975192.300000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature: sups was rescaled by a factor of 10000.0\n",
      "The feature: moff was rescaled by a factor of 100000.0\n",
      "The feature: mon was rescaled by a factor of 1000000.0\n",
      "IMPORTANT: Take these rescaling in account when reading the plots\n"
     ]
    }
   ],
   "source": [
    "for feature in data.columns:\n",
    "    if data[feature].values.dtype != 'object':\n",
    "        scale_factor = np.round(np.log10(np.mean(data[feature])))\n",
    "        data[feature] = data[feature] / 10**scale_factor\n",
    "        print 'The feature: '+feature+' was rescaled by a factor of '+str(10**scale_factor)\n",
    "print 'IMPORTANT: Take these rescaling in account when reading the plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'sups', u'moff', u'mon'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sups</th>\n",
       "      <th>moff</th>\n",
       "      <th>mon</th>\n",
       "      <th>hd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>0.4246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041934</td>\n",
       "      <td>NewYearsDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>0.6569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054117</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>0.7466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-04</th>\n",
       "      <td>0.6911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047323</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-05</th>\n",
       "      <td>0.5929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048324</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sups  moff       mon           hd\n",
       "event_date                                     \n",
       "2014-01-01  0.4246   NaN  0.041934  NewYearsDay\n",
       "2014-01-02  0.6569   NaN  0.054117          NaN\n",
       "2014-01-03  0.7466   NaN  0.051634          NaN\n",
       "2014-01-04  0.6911   NaN  0.047323          NaN\n",
       "2014-01-05  0.5929   NaN  0.048324          NaN"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some statistics\n",
    "print data.columns[data.columns!='hd']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1) \t CORRELATION \t\n",
    "\n",
    "## Q1.1) Using Python and the libraries and packages of your own choice, write a well-structured and readable program that can be used to analyse the data set for correlations between the time series. The program should work with data files that are similar, but not necessarily identical to the sample provided. Your response should include the program itself and brief instructions about any dependencies that should be installed before running it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal\n",
    "\n",
    "\n",
    "The idea behind the solution of this first question is to provide the user the most usefol plots and information to observe any correlation. With that in mind after the execution of \"correlation_study\" the next plots will be presented on the screen (different windows):\n",
    "* Temporal distribution by year. The idea of this plot is to provide the user a broad overview of the activity of the different features. For that reason each feature has assigned one colum while the rows represent different distributions:\n",
    "    * First row, distribution of features accross the year for different years.\n",
    "    * Second row, distribution of the features per month for different years. \n",
    "    * Third row, distribution of the featyres during the weekdays for different years. \n",
    "\n",
    "* Monthly distribution of features. In this plot I present the monthly distribution of the different features. The idea here is to provide some insight into the data before the next plot. \n",
    "* Monthly features crosscorrelations. This plot address explicitly the question of the correlation between the different time series of the data set in the temporal scale of a month. I did it in this way to address that the data is non-stationary and hence this relationship depends strongly of the period of the year (month). Take in account that the first zero lag is from the first time series mentioned in the plot to the second. Then if you observed a peak in the positive side of the crosscorrelograms should be interpreted as a larger probability of having the event two in that lag of time respect the first time series mentioned in the plot. The cross correlations are computed to have the same number of samples than the original time series (I find a 'full' version non informative in this time scale). Whenever you find an empty plot is because there was not enough collected data to calculate any statistics. \n",
    "\n",
    "* Rolling corelation coefficient and weekly activity of the different features. In my opinion this is the more informative than the previous one but address the problem a bit different. This figure shows the moving correlation coefficient (with a temporal window of 14 days, 2 weeks) together with the dynamics of the features (resampled to weekly activity for the sake of smooth it). Interestingly in this way it is possible to extract some statistics on what it is going with this coefficient divided by years. This is shown in the last row of the plot. This row presents the distributions of the correlations during the year and provide a nice picture of the business choices taken during that year. It also measure the effectiveness of whatever event because this will have an impact in the shape and other moments of those distributions. The median (assuming that in general the situation wont be gaussian) is shown as a parameter for decision making (or index of correlation for a year). In the cases the skweness of the distribution is close to a gaussian, a normal distribution will be fitted to the data (and the mean and variance of the distribution will be shown). In addition to this in the console there will be reflected also the activity of the different moments of the distributions per year. \n",
    "\n",
    "These plots are just tools, a window to the observer to have an insight on what it is happening with the correlation of the features. \n",
    "\n",
    "Non-numeric features will not be considered for this study. That is the reason for discarding (not using) the holydays.\n",
    "\n",
    "In the next cells I present the same code that is used in the function that will help in the reporting of the specific case of study.\n",
    "\n",
    "There is some redundancy in the code but this was done with the idea of reusability of blocks of code, not only for me but for other developers too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirota/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called values(...) on this deferred object which materialized it into a series\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# Computation of the distributions per year for the different features\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "\n",
    "# Definition of the figure.\n",
    "figid = plt.figure('Temporal distributions by year', figsize=(20, 10))\n",
    "\n",
    "# Definition of the matrix of plots. In this case the situation is more complex that is why I need to define a\n",
    "# matrix. It will be a dim[2x3] matrix.\n",
    "col = len(data.columns[data.columns!='hd'])\n",
    "gs = grds.GridSpec(3,col)\n",
    "\n",
    "for i in range(col):\n",
    "    \n",
    "    ax1 = plt.subplot(gs[0, i])\n",
    "    ax2 = plt.subplot(gs[1,i])\n",
    "    \n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)].values\n",
    "        ax1.plot(np.arange(len(dat)),dat,'-')\n",
    "        legend.append(str(y))\n",
    "    ax1.legend(legend)\n",
    "    ax1.set_title('Daily '+data.columns[i])\n",
    "\n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)].resample('M').values\n",
    "        ax2.plot(np.arange(len(dat))+1,dat,'-o')\n",
    "        legend.append(str(y))\n",
    "    ax2.legend(legend)\n",
    "    ax2.set_title('Monthly '+data.columns[i])\n",
    "    plt.xlim([1,12])\n",
    "\n",
    "    ax3 = plt.subplot(gs[2,i])\n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)]\n",
    "#         dat = dat.groupby(data.index.dayofweek).mean()\n",
    "        dat = dat.groupby(dat.index.dayofweek).mean()\n",
    "        dat.index=['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']\n",
    "        dat.plot(style='-o')\n",
    "        legend.append(str(y))\n",
    "    ax3.legend(legend)\n",
    "    ax3.set_title('Day week '+data.columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *****************************************************\n",
    "# Computation of the distribution of activity per month for the different time series that are present in the data\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "\n",
    "# Definition of the figure.\n",
    "figid = plt.figure('Monthly distribution of features',figsize=(20, 10))\n",
    "\n",
    "# Definition of the matrix of plots. In this case the situation is more complex that is why I need to define a\n",
    "# matrix. It will be a dim[2x3] matrix.\n",
    "col = len(data.columns[data.columns!='hd'])\n",
    "rows = len(years) \n",
    "gs = grds.GridSpec(rows,col)\n",
    "months=['Jan','Feb','Mar','Aprl','May','Jun','Jul','Agst','Sep','Oct','Nov','Dec']\n",
    "colors = sns.hls_palette(12, l=.5, s=.6)\n",
    "\n",
    "for c in range(col):\n",
    "    for r in range(rows):\n",
    "        ax1 = plt.subplot(gs[r, c])\n",
    "\n",
    "        dat_year = data[data.columns[c]][str(years[r])]\n",
    "\n",
    "        for m in range(1,13):\n",
    "#             print m\n",
    "            dat = dat_year[dat_year.index.month==m].values\n",
    "            ax1.plot(np.arange(len(dat)),dat,'-', color=colors[m-1])\n",
    "        if r==0 and c==col-1:\n",
    "            ax1.legend(months, bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0.,ncol=2, fancybox=True,frameon=True)\n",
    "  \n",
    "        if c==0:\n",
    "            ax1.set_ylabel('Year: '+str(years[r]))\n",
    "            \n",
    "        if r==0:\n",
    "            ax1.set_title('Feature: '+str(data.columns[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirota/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/sirota/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# Computation of the distribution of cross correlations per month for the different time series that are present in the data\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "\n",
    "# Definition of the figure.\n",
    "figid = plt.figure('Monthly features cross-correlations', figsize=(20, 10))\n",
    "\n",
    "# Definition of the matrix of plots.\n",
    "feature1= [2,1,1]\n",
    "feature2 =[0,0,2]\n",
    "\n",
    "\n",
    "col = len(data.columns[data.columns!='hd'])\n",
    "rows = len(years) \n",
    "\n",
    "gs = grds.GridSpec(rows,col)\n",
    "months=['Jan','Feb','Mar','Aprl','May','Jun','Jul','Agst','Sep','Oct','Nov','Dec']\n",
    "# colors = sns.color_palette(\"Set2\", 12)\n",
    "colors = sns.hls_palette(12, l=.5, s=.6)\n",
    "\n",
    "for c in range(col):\n",
    "    for r in range(rows):\n",
    "        ax1 = plt.subplot(gs[r, c])\n",
    "\n",
    "        dat_year_feat1 = data[data.columns[feature1[c]]][str(years[r])]\n",
    "        dat_year_feat2 = data[data.columns[feature2[c]]][str(years[r])]\n",
    "        \n",
    "        for m in range(1,13):\n",
    "            dat_feat1 = dat_year_feat1[dat_year_feat1.index.month==m].values\n",
    "            dat_feat1= np.subtract(dat_feat1,np.mean(dat_feat1))\n",
    "            dat_feat2 = dat_year_feat2[dat_year_feat2.index.month==m].values\n",
    "            dat_feat2= np.subtract(dat_feat2,np.mean(dat_feat2))\n",
    "            dat = sgn.correlate(dat_feat1,dat_feat2,mode='same')\n",
    "            ax1.plot(np.linspace(-15,15,len(dat)),dat,'-', color=colors[m-1])\n",
    "        if c==0:\n",
    "            ax1.set_ylabel('Year: '+str(years[r]))\n",
    "        if r==0 and c==col-1:\n",
    "            ax1.legend(months, bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0.,ncol=2, fancybox=True,frameon=True)\n",
    "        if r==0:\n",
    "            ax1.set_title('Xcorr: '+str(data.columns[feature1[c]])+' and '+str(data.columns[feature2[c]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirota/anaconda2/lib/python2.7/site-packages/matplotlib/cbook.py:2534: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called index(...) on this deferred object which materialized it into a series\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n",
      "  return y.index.values, y.values\n",
      "/home/sirota/anaconda2/lib/python2.7/site-packages/matplotlib/cbook.py:2534: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called values(...) on this deferred object which materialized it into a series\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n",
      "  return y.index.values, y.values\n",
      "/home/sirota/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: pd.rolling_corr is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=14).corr(other=<Series>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Correlation distribution for year 2014\n",
      "Mean: 0.532746314132\n",
      "Median: 0.557949258657\n",
      "Standard deviation: 0.215720671611\n",
      "Kurtosis: -0.042142352798\n",
      "Skewness: -0.638196099596\n",
      "Normal distribution fitted!\n",
      "mu=0.532746314132\n",
      "sigma=0.215414032264\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2015\n",
      "Mean: 0.56389935887\n",
      "Median: 0.635166988321\n",
      "Standard deviation: 0.259349079152\n",
      "Kurtosis: 0.449585874116\n",
      "Skewness: -1.00510390293\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2016\n",
      "Mean: 0.378524397416\n",
      "Median: 0.417440043376\n",
      "Standard deviation: 0.326511914571\n",
      "Kurtosis: 0.0600812000585\n",
      "Skewness: -0.535276902702\n",
      "Normal distribution fitted!\n",
      "mu=0.378524397416\n",
      "sigma=0.326049105104\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2017\n",
      "Mean: 0.407220906323\n",
      "Median: 0.435383118459\n",
      "Standard deviation: 0.197383216262\n",
      "Kurtosis: -0.00269736097945\n",
      "Skewness: -0.699672583568\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2014\n",
      "Mean: nan\n",
      "Median: nan\n",
      "Standard deviation: nan\n",
      "Kurtosis: nan\n",
      "Skewness: nan\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2015\n",
      "Mean: nan\n",
      "Median: nan\n",
      "Standard deviation: nan\n",
      "Kurtosis: nan\n",
      "Skewness: nan\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2016\n",
      "Mean: 0.0667764147975\n",
      "Median: 0.0597625619621\n",
      "Standard deviation: 0.305620922964\n",
      "Kurtosis: 0.359789556375\n",
      "Skewness: -0.277127492116\n",
      "Normal distribution fitted!\n",
      "mu=0.0667764147975\n",
      "sigma=0.304892385758\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2017\n",
      "Mean: 0.0586522881211\n",
      "Median: 0.00137535800231\n",
      "Standard deviation: 0.249063699487\n",
      "Kurtosis: -0.205005171739\n",
      "Skewness: 0.743828219699\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2014\n",
      "Mean: nan\n",
      "Median: nan\n",
      "Standard deviation: nan\n",
      "Kurtosis: nan\n",
      "Skewness: nan\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2015\n",
      "Mean: nan\n",
      "Median: nan\n",
      "Standard deviation: nan\n",
      "Kurtosis: nan\n",
      "Skewness: nan\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2016\n",
      "Mean: 0.0953310356166\n",
      "Median: 0.125141885314\n",
      "Standard deviation: 0.291574952557\n",
      "Kurtosis: -0.247701590369\n",
      "Skewness: -0.385902341337\n",
      "Normal distribution fitted!\n",
      "mu=0.0953310356166\n",
      "sigma=0.290879898046\n",
      "-------------------------------------------------\n",
      "Correlation distribution for year 2017\n",
      "Mean: -0.126156754847\n",
      "Median: -0.0595728797088\n",
      "Standard deviation: 0.429693681993\n",
      "Kurtosis: -1.05975389502\n",
      "Skewness: -0.149630819619\n",
      "Normal distribution fitted!\n",
      "mu=-0.126156754847\n",
      "sigma=0.424997435171\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# Computation of the distribution of activity per month for the different time series that are present in the data\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "# Definition of the matrix of plots.\n",
    "feature1 = [0,0,2]\n",
    "feature2 = [2,1,1]\n",
    "\n",
    "for f in range(len(feature1)):\n",
    "\n",
    "    figid = plt.figure(\n",
    "        'Rolling correlation coefficient and weekly activity of ' + data.columns[feature1[f]] + ' and ' + data.columns[\n",
    "            feature2[f]], figsize=(20, 10))\n",
    "    rows = len(years)\n",
    "\n",
    "    gs = grds.GridSpec(4, 4)\n",
    "\n",
    "    for r in range(rows):\n",
    "        ax1 = plt.subplot(gs[0, :])\n",
    "        ax2 = plt.subplot(gs[1, :])\n",
    "        ax3 = plt.subplot(gs[2, :])\n",
    "\n",
    "        dat_year_feat1 = data[data.columns[feature1[f]]][str(years[r])]\n",
    "        dat_year_feat2 = data[data.columns[feature2[f]]][str(years[r])]\n",
    "\n",
    "        ref = ax1.plot(dat_year_feat1.resample('W'))\n",
    "        ax2.plot(dat_year_feat2.resample('W'))\n",
    "\n",
    "        xcorr = pd.rolling_corr(dat_year_feat1, dat_year_feat2, 14)\n",
    "        ax3.plot(xcorr)\n",
    "\n",
    "        ax4 = plt.subplot(gs[3, r])\n",
    "        n, bins, patches = ax4.hist(xcorr.values[np.logical_not(np.isnan(xcorr.values))], bins=np.round(len(xcorr) / 6),\n",
    "                                    facecolor=ref[0].get_color(), edgecolor=ref[0].get_color())\n",
    "        mediana = ax4.axvline(np.median(xcorr.values[np.logical_not(np.isnan(xcorr.values))]), color='r', linestyle='--')\n",
    "        ax4.set_xlim([-1, 1])\n",
    "        ax4.set_xlabel('CorrCoef year '+str(years[r]))\n",
    "        ax4.set_label(mediana)\n",
    "\n",
    "        print '-------------------------------------------------'\n",
    "        print 'Correlation distribution for year ' + str(years[r])\n",
    "        print 'Mean:', xcorr.mean()\n",
    "        print 'Median:', xcorr.median()\n",
    "        print 'Standard deviation:', xcorr.std()\n",
    "        print 'Kurtosis:', xcorr.kurtosis()  # Kurtosis is mainly related with outliers not with the central peak\n",
    "        print 'Skewness:', xcorr.skew() #Take in account that this value (has not) HAS the substraction of the skew of a normal distribution (3)\n",
    "\n",
    "        if (np.abs(xcorr.skew())) < 0.65:\n",
    "            mu, sigma = stat.norm.fit(xcorr.values[np.logical_not(np.isnan(xcorr.values))])\n",
    "            print 'Normal distribution fitted!'\n",
    "            print 'mu=' + str(mu)\n",
    "            print 'sigma=' + str(sigma)\n",
    "\n",
    "            fitted_normal = mlab.normpdf(bins, mu, sigma) * np.max(\n",
    "                xcorr.values[np.logical_not(np.isnan(xcorr.values))])\n",
    "            # print fitted_normal\n",
    "            normfit = ax4.plot(bins, fitted_normal * np.max(n), 'r--', linewidth=2, color=\"#3498db\")\n",
    "            ax4.legend(['Median '+str(round(xcorr.median(),2)), 'N(' + str(round(mu, 1)) + ',' + str(round(sigma, 2)) + ')'], loc='best')\n",
    "        else:\n",
    "            ax4.legend(['Median '+str(round(xcorr.median(),2))], loc='best')\n",
    "\n",
    "\n",
    "    ax1.set_ylabel(data.columns[feature1[f]])\n",
    "    ax2.set_ylabel(data.columns[feature2[f]])\n",
    "    ax3.set_ylabel('Rolling correlation, 14 days period')\n",
    "\n",
    "\n",
    "    ax1.legend(years, bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0., ncol=1, fancybox=True, frameon=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#    Q1.2) Run the program on the provided data sample from PetFood and comment on the output.\n",
    "\n",
    "The analysis of this data is quite interesting.\n",
    "\n",
    "The signups distribution along the year suffer from some seasonality in the years with only online adds. When the new offline adds started the distribution is completely different and the total amount of sign ups is increased what looks like 20%. The same happend with the distribution during the days of the week. But with less impact and having always the friday as the larger sign up day. \n",
    "\n",
    "Comparing the two methods of advertisements the online seems a stable investment of money with a better prediction on the inpact of the sign ups than the offline methods. These ones consume a big budget of investment and do not provide a correlation with the activity of the sign ups. \n",
    "\n",
    "The monthly distribution of all the features shows this situation too. The peaks if sign ups were well stablished before the offline add, but seem to vanish and shows more noisy picture in the last two years.\n",
    "The investment on online add is really seasonal having usualy minimums during the first month of the year. \n",
    "\n",
    "\n",
    "The monthly crosscorrelations show clear that the online services are moderate correlated with the sign ups (response of the customer) during the whole year, following a period of 5 days. The situation is a bit different, though, for november which present high(moderate high for 2014) degree of correlation. In addition the activity of the sign ups is after the zero which indicates a possibility of causation (warning!) of the online add on the behaviour of the consumers. \n",
    "\n",
    "The offline shows much noisy uncorrelated picture. There is no structure (periodic) on the crosscorrelogram and the activity of the customers seems to happend BEFORE the major investments on these adds. This is interesting. It could be a proof that the campaign on those media is failing, going fater the noisy behavior of the market or the will of a non expert. It seems like after checking slightly/substantial increasings in signups with moderate spends on the offline, the company decided to invest more or invest in a way less atractive to the customer. It could be also a saturation effect on the decision of the customer that suffer from unnecesary adds on TV. This is specially clear in November but it is a general trend. \n",
    "\n",
    "The relation between the two ways of adds seems uncorrelated except for november 2016 where probably due to the christmas campaing the company invest more money in both...but having only major effect on the customer at the level of the online media. \n",
    "\n",
    "The plots of the rolling cross correlation coefficient shows a tendency of better performance and predictability of customer choices with the online adds. The median of the correlation was higher for 2014 and 2015 while there is a decrease of this when the offline add started (2016,2017). Then it is not only that the offline is not really predicting the behaviour of the customer but it seems (well, this is a causal declaration but the data point towards that) that is decreasing the correlation of the customer behavior based on the online media. \n",
    "\n",
    "Summarizing, despite the total increasing of sign ups specially after the big first investment. This ofline campaign seems not useful for predicting the behavior of the customers. The first two years the nice are nice and according to the distributions of signups the situation seems to be fitting a Poisson process which is really convenient when one has to scale system (serers, services, etc) with the general queue theory. The new offline adds disrupted this situation and made it more difficult to predict putting almost at random level the investment with the effect on the customer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     Q1.3) Comment on additional approaches that could be used to search for various types of correlations in the data set.\n",
    "\n",
    "Further ideas that one can use here for correlations:\n",
    "\n",
    "* Study of the cross-spectral density and coherence\n",
    "* Study of granger causality\n",
    "* Study of PCA/ICA-ish analysis on the time series to see any pattern that coud be capturing the variance in a meaningful way\n",
    "* Use of the statsmodels package to apply some statistical test on the correlation http://statsmodels.sourceforge.net/ I am currently exploring this toolbox but I could not extract the whole potential\n",
    "* Split the offline spends based on the type of media (TV,newspaper, etc). Probably these media has really different approach from the customer behavior side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2) SEASONS AND CYCLES \t\n",
    "## Q2.1) Using Python and the libraries and packages of your own choice, write\ta well-structured and readable program that can be used to identify periodic behaviour in the “signups” time series. The program should work with data files that are similar, but not necessarily identical to the sample provided. Your response should include the program itself and brief instructions about any dependencies that should be installed to run it.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal\n",
    "\n",
    "The idea behind the solution the second question is to provide the user the most useful plots and information to observe any seasonality or cycles in the data. With that in mind after the execution of \"seasons_study\" the next plots will be presented on the screen (different windows):\n",
    "\n",
    "* Temporal Sign-ups distributions by year. The purpose of this plot is to give certain broad view on the data. It is also used in the previous function because helps to visualize the activity of the different years and why is convenient to split the analysis per year (due to the non-stationarity of the processes). Each row represents:\n",
    "    * First row, the daily activity of the sign ups during the period of one year (color coded per year).\n",
    "    * Second row, the monthly activity of the sign ups. This provides a broader view that happend in a wider time scale; ignoring the noisy daily signal some insights could be observed.\n",
    "    * Third row, the activity of sign ups during the week days. This is a small time scale but helps to see the relevance of certain days of the week. \n",
    "    \n",
    "* Spectral estimators per year. This plots is the core of the idea to capture the seasons and cycles. It is based on the well-known Fourier analysis. Eacho row represent the espectral estimators for each year. The colums represent:\n",
    "    * The power spectral density of the signal (PSD). This concept capture the general distribution of power of the signal during the observation of time (one year in this case). It provides a first estimator of which spectral components are relevant for the time series. The peaks in frequency represent periods (seasons) of the signal.\n",
    "    * Spectrogram. This spectral estimator provides slightly more insight in the temporal components of the time series in the frequency domain. Using a sliding window one can compute the Fourier transform on those windows and chunking them together while the window is moving, presenting a 2-D map of time x frequency (x axis for time, y axis for frequency). Using a window of 32 days (samples, just for convenience of being a power of 2) and overlapping each window 80% of the neighbour windows I provide a powerful tool for evaluating the periodicity. \n",
    "    * Log10 scale spectrogram. This colum is similar to the previous but in log10 scale to provide no so crispy visual input and observe other components. \n",
    "\n",
    "The DC components (f=0) is not shown on the PSD.\n",
    "\n",
    "These plots are just tools, a window to the observer to have an insight on what it is happening with the sign up periodicity. \n",
    "\n",
    "In the next cells I present the same code that is used in the function that will help in the reporting of the specific case of study.\n",
    "\n",
    "There is some redundancy in the code but this was done with the idea of reusability of blocks of code, not only for me but for other developers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirota/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called values(...) on this deferred object which materialized it into a series\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# Computation of the distributions per year for sign ups\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "\n",
    "# Definition of the figure.\n",
    "figid = plt.figure('Temporal Sign-ups distributions by year', figsize=(20, 10))\n",
    "\n",
    "# Definition of the matrix of plots. In this case the situation is more complex that is why I need to define a\n",
    "# matrix. It will be a dim[2x3] matrix.\n",
    "col = len(data.columns[data.columns=='sups'])\n",
    "gs = grds.GridSpec(3,col)\n",
    "\n",
    "for i in range(col):\n",
    "    \n",
    "    ax1 = plt.subplot(gs[0, i])\n",
    "    ax2 = plt.subplot(gs[1,i])\n",
    "    \n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)].values\n",
    "        ax1.plot(np.arange(len(dat)),dat,'-')\n",
    "        legend.append(str(y))\n",
    "    ax1.set_title('Daily '+data.columns[i])\n",
    "\n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)].resample('M').values\n",
    "        ax2.plot(np.arange(len(dat))+1,dat,'-o')\n",
    "        legend.append(str(y))\n",
    "    ax2.set_title('Monthly '+data.columns[i])\n",
    "    plt.xlim([1,12])\n",
    "\n",
    "    ax3 = plt.subplot(gs[2,i])\n",
    "    legend =[]\n",
    "    for y in years:\n",
    "        dat = data[data.columns[i]][str(y)]\n",
    "        dat1 = dat.groupby(dat.index.dayofweek).mean()    \n",
    "        dat1.index=['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']\n",
    "        dat1.plot(style='-o')\n",
    "        \n",
    "        legend.append(str(y))\n",
    "    ax3.set_title('Day week '+data.columns[i])\n",
    "    \n",
    "    if i==0:\n",
    "        ax1.legend(years, bbox_to_anchor=(1, 1), loc='best', borderaxespad=0.,ncol=1, fancybox=True,frameon=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs 1.15740740741e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirota/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:51: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************\n",
    "# Computation of different spectral estimators per year for sign ups\n",
    "years = np.unique(data.index.year)\n",
    "\n",
    "\n",
    "# Definition of the figure.\n",
    "figid = plt.figure('Spectral estimators per year32', figsize=(20, 10))\n",
    "\n",
    "# Definition of the matrix of plots.\n",
    "col = 4\n",
    "rows = len(years)\n",
    "gs = grds.GridSpec(rows,col)\n",
    "\n",
    "#Spectral parameters\n",
    "Window_length = 32 #in days\n",
    "# This is equivalent to multitaper estimation but just one taper.\n",
    "window = sgn.slepian(Window_length, width=0.3)\n",
    "overlap = 0.85\n",
    "T1= 24*60*60 # 1 day in seconds\n",
    "\n",
    "fs = 1./T1\n",
    "print 'fs',fs\n",
    "\n",
    "   \n",
    "\n",
    "for r in range(rows):\n",
    "    ax01 = plt.subplot(gs[r, 0])\n",
    "#     ax02 = plt.subplot(gs[r, 1])  \n",
    "    ax1 = plt.subplot(gs[r, 1])\n",
    "    ax2 = plt.subplot(gs[r, 2])\n",
    "    ax3 = plt.subplot(gs[r, 3])\n",
    "\n",
    "    dat = data['sups'][str(years[r])].values\n",
    "    \n",
    "    nfft=128\n",
    "    if r==3:\n",
    "        nfft=64\n",
    "    \n",
    "#     MODIFICATION with respect the original exercise\n",
    "#     For the sake of clarity I just substract the \"DC\" component, the baseline or offset\n",
    "#     spec = np.fft.fft(dat,n=nfft) #Original line\n",
    "    spec = np.fft.fft(dat-np.mean(dat),n=nfft)\n",
    "    n = nfft\n",
    "    f = np.fft.fftfreq(n,T1)\n",
    "    \n",
    "\n",
    "    bins = np.ceil(np.arange(nfft/2))\n",
    "    bins = bins.astype(int)\n",
    "    peri = bins*fs/nfft\n",
    "\n",
    "    ax01.plot(bins,((1./peri))/T1,'.-')\n",
    "    ax01.set_xlim([bins[0],bins[-1]])\n",
    "        \n",
    "    \n",
    "#     ax1.plot(np.abs(spec[0:np.ceil(len(spec)*1.0/2)]))\n",
    "    ax1.plot(np.abs(spec[bins]))\n",
    "    ax1.set_xlim([bins[0],bins[-1]])\n",
    "\n",
    "    f, t, Sxx = sgn.spectrogram(dat-np.mean(dat),nfft=nfft,fs=fs,nperseg=Window_length,window=window,noverlap=np.round(Window_length*overlap))\n",
    "    \n",
    "\n",
    "    spec= np.abs(Sxx)\n",
    "    \n",
    "    ax2.imshow(spec,vmax=np.percentile(spec,95),aspect='auto',cmap='viridis',interpolation='none',extent=[0,365,nfft/2,0])\n",
    "    ax3.imshow(np.log(spec),vmax=np.percentile(np.log(spec),95),aspect='auto',cmap='viridis',interpolation='none',extent=[0,365,nfft/2,0])\n",
    "\n",
    "    \n",
    "    \n",
    "    ax2.set_ylabel('Year: '+str(years[r])+'\\nbins',multialignment='center')\n",
    "    \n",
    "    if r==0:\n",
    "        ax01.set_title('Time frequency realationship to interpret the ')\n",
    "        ax1.set_title('PSD[n.u.]')\n",
    "        ax2.set_title('Spectrogram[n.u.]')\n",
    "        ax3.set_title('Spectrogram[log10]')\n",
    "\n",
    "    if r==rows-1:\n",
    "        ax1.set_xlabel('bins')\n",
    "        ax2.set_xlabel('Time[days of the year]')\n",
    "        ax3.set_xlabel('Time[days of the year]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.2) Run the program on the data sample from PetFood, and comment on\tthe output\n",
    "\n",
    "Once the function is called the results point clear to certain level of seasonality/ periodicity similar for the two first years and different for the last two years.\n",
    "\n",
    "The temporal distributions per year shows that the 2014 and 2015 have a quite similar behaviour in the monthly scale and also in the day of the week. Two troughs can be observed in april/ may and october, while two peaks are visible at february and june. The first year with the new offline adds shows also two troughs and two peaks, the earliers as before april/may and october, but the later, the peaks, with a slightly shift now at february and july. \n",
    "\n",
    "Observing the PSD per year it is possible to observe peaks of activity at 0.15 $\\mu Hz$ and 0.28 $\\mu Hz$ for 2014. With a $f_s = \\frac{1}{1day} Hz$ that is $f_s=11.574 \\mu Hz$  this represent for the first case 1 cycle every 2.7 months, while the other peak means 1 cycle every 1.3 months. Despite of the peaks the PSD seems to have the main power concentrated around 0.1-0.09 which will mean 1 cycle every 3.8-4.2 months. This last observation is compatible with the observation in the previous plot. \n",
    "\n",
    "# <font color='red'>Modification. </font>\n",
    "\n",
    "I was confused with the axis and the frequency temporal transformation. I corrected in the new calculations and I add a plot to translate the number of bin into days. This relationship is given by $binx[\\frac{f_s}{nfft}]$. In these new calculations I took n=128 except for the last year nfft=64. The sampling rate was correct (sampling every day) that is $f_s=11.574 \\mu Hz$. To translate this to days I need $binx[\\frac{1}{nfft}*T_s]$ which is not lineal with respect the bin number.\n",
    "\n",
    "\n",
    "At this point I prefer to plot the frequency axes in number of bins (horizontal axis for the PSD and vertical axis for the spectrograms) rather than in frequency (which at this time scale seems difficult to interpret) or a non-linear temporal scale. That is why I plot in the first column the function that relates the number of bin with the period in time(instead of frequency). Perhaps this is not common (at least is not common in my current field of knowledge but I work in a less than 1s scale....then is easier to interpret the Hz) but with this time scale I prefer this solution rather than the other two options for the sake of clarity to the customer. Then the customer or the person that has to analyze the data can zoom in/out in this plot to see the correspondence between the number of bin (horizontal axis) with the number of days(vertical axis). \n",
    "\n",
    "\n",
    "Taking this in account the previous explanation change dramatically and I apologize for this mistake, because in deed it was obvious from the crosscorrelations of the previous exercise that the period is in the range of days not months. Then:\n",
    "\n",
    "* For 2014, there are two relevant peaks at the PSD, one at bin=1 (~120 days) and the other between bin=18 (this means ~6.8 days).The peak at 120 days reflects the tendency that is also visible in the temporal distribution of sign ups per month. In that plot it is possible to see how the wave shape is almost sinusoidal of period ~4 months. This periodicity does not show up in the spectrograms because those are computed with a window of 32 days and hence it can not capture events larger than that (for capturing then one should use a larger window).\n",
    "\n",
    "    The periodicity at ~6.8 it is capture in both the PSD and the spectrogram and shows how relevant this band is except at two moments of the year, June and December. In June there is a peak at bin=30 (~4 days, this is the duration of the peak observed in the time series at that point. In November/December there is a peak at bin= 40 (~3 days) and another one a bit delayed at bin =10 (~11 days).\n",
    "    \n",
    "    All together the most important trends in this year are the one lasting 120 days (4 months) and the 6.8 days. \n",
    "\n",
    "* For 2015, we can observed from the PSD two peaks in the long temporal range at bin=1 (~120 days) and bin=5 (~25 days). This is an interesting change in the tendency that the Fourier analysis provides and is dificult to grasp at the time series. The band at bin=18 (~6.8 days)(with a peak in the PSD and a band in the spectrogram) is still relevant but observing the spectrograms it does not look as relevant as the previous year. At January there is a relevant peak but this is not stable throughout the year and itvanishs after february. \n",
    "    The previous peak in Juni shifted to a shorter time scale being the peak at bin =40 (~3 days). The situation in November/December is kind of similar but with one first event at bin ~7 (~18 days) and another one a bit layed at ~3 days.\n",
    "    \n",
    "* For 2016, the amplitude of the ~120 days peak from the PSD is even larger, such that obscure also other peaks (one thing to notice is that despite the period is the same, checking the temporal plot for the months it is obvious that the phase changed/shifted). The next peak is at bin=5 (~25 days); from the spectrograms this is specially relevant at the begining of the year but not after March. In this month there is a big deplexion of the power visible at the spectrograms. This deplexion mix three four bands bin=5(~25 days), bin=12 (~11 days), bin=15 (~8 days), bin =35 (~3.5 days) and bin=55 (~2.5 days). This mixture of pattern match a big investment in the Online advertisement.\n",
    "    After this point the year oscillates between the bands of bin=15(~8 days) and bin=5(~25 days). \n",
    "    The event that before was in end of November seems anticipated one month to October having at that point two clear bands at bin=5(~25 days) and bin=18(~7 days). \n",
    "    In December there is also a peak of activity in the bands of bin=5(~25 days) and bin=35 (~3.5 days).\n",
    "    \n",
    "* For 2017, there is information for only two months but so far the tendency is to have a wide peak at bin=7 (~9.5 days) specially at the end of February. There is as the PSD points, a tendency of having a peak of activity at bin =1 (~30 days)\n",
    "\n",
    "\n",
    "This results shows that the method is valid to capture the seasons of the sign ups time series. \n",
    "# <font color='red'>End of the modification. </font>\n",
    "\n",
    "The PSD for 2015 shows less spiky activity but rather more power at 0.1 with a small peak at 0.28. It is important to note also that a lot of power, as also happened before with 2014, it is located around 0.05 which it translate in a cycle every, aprox, 7 months. This could suggest a general trend in that range.  \n",
    "\n",
    "The PSD of 2016 shows also this weekly periodicity at around 7 days. The peak at 0.15 have changed here to a wider distribution, more peaky at 0.128 which means 1 cycle every 3 months (slower). The peak at 0.28 seems still present though. \n",
    "\n",
    "The PSD of 2017 has so few samples that is difficult to consider the estimator as good. Nonetheless one can take a llok and check the trends. The trend is that there is a distribution around 0.1 which means 3.85 months.\n",
    "\n",
    "Observing the spectrograms the temporal seasons of the spectral componenets are much more clear. 2014 and 2015 share a common tendency of having a season of 0.1 at may/june and november. 2016 despite of having that compoonent too (a bit slower) it show it in april and november. Despite of that the three years have a clear component in the season befor christmas, having almost the same (a bit shifted) components for the range of november/december. \n",
    "\n",
    "\n",
    "This results shows that the method is valid to capture the seasons of the sign ups time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.3) Discuss any additional methods and data sources that would be useful to improve the detection of cycles in the number of signups.\n",
    "\n",
    "Fruther methods that could be explored:\n",
    "\n",
    "\n",
    "* Multitaper estimator, this provides a better estimator.\n",
    "* Wavelet estimator of the spectrum\n",
    "* Phase relationships with Hilbert transform as support to extract the phase \n",
    "* The ideas of this paper, \"Brain Oscillations and the Importance of Waveform Shape\" http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(16)30218-2?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1364661316302182%3Fshowall%3Dtrue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q2.4) Discuss to what degree this same code solution can be expected to\twork for a completely different customer, selling a\tcompletely different product, in a different market. Would the approach\tneed to be adjusted to accommodate such a general setting?\n",
    "\n",
    "The use of Fourier transform and the spectogram is a general approach to any time series. In that sense the method is agnostic to the customer, product or market; hence it is validity is appropriate for the task of getting insights into the cyclic dynamics of any system.\n",
    "\n",
    "Despite of that the main concern that one should take when dealing with this spectral estimators is the method that one used because the parameters could have a big impact in the representation of the estimator. This happens also with the wavelet transform and is the fact of having trade off when using the technique. One critical point is the window for the spectrogram because there should be big enough to capture enough degrees of freedom but narrow to fit the characteristic of the oscillations that one wants to observe. The bias and variance of the estimator is also a matter of debate. \n",
    "\n",
    "None the less these are technical details that do not affect the customer. Only the data scientist must be aware of this situation and have wide open eyes to do not let these effects uncontrolled. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
